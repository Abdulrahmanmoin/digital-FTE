---
type: tweet
source: x_twitter
tweet_id: "2020558818103439584"
author_username: "TheAhmadOsman"
author_name: "Ahmad"
author_id: ""
tweet_type: "keyword_match"
detection_source: "search"
conversation_id: ""
created_at: "2026-02-08T18:02:17.000Z"
received_at: "2026-02-08T23:06:17.014027"
status: pending
---

# Tweet: keyword_match from @TheAhmadOsman

## Metadata
| Field      | Value |
|------------|-------|
| Author     | @TheAhmadOsman (Ahmad) |
| Tweet ID   | 2020558818103439584 |
| Type       | keyword_match |
| Source     | search |
| Created    | 2026-02-08T18:02:17.000Z |
| Conversation ID |  |

## Tweet Content
pretty significant, vllm has also improved tokens/sec a lot

if your inference engine supports the anthropic api, you can hook it straight into claude code (vllm does this out of the box)

i oneshot an openai to anthropic api proxy , very easy as well

## Referenced Tweets
None

## Matched Keyword
Claude Code

## Raw Reference
- Tweet ID: `2020558818103439584`
- Author ID: ``
- Detection source: `search`
