---
type: linkedin_post_action
source_task: "scheduled_post"
status: pending_approval
---

# Proposed LinkedIn Post

The hardest part of building AI agents isn't the AI — it's the plumbing.

Everyone talks about prompts and models. But the real engineering challenge is everything around the model: how does the agent perceive its environment? How does it hand off decisions to humans? How does it recover when something breaks at 3am?

I've been building a system I call a "Digital Employee" — a collection of long-running processes that monitor Gmail, LinkedIn, X, and Facebook, reason about what needs attention, and surface decisions for human approval before taking any action.

Here's what I've learned:

Files beat APIs for agent coordination. A folder full of markdown files is auditable, debuggable, and survives restarts. A Redis queue is fast but opaque.

Human-in-the-loop is a feature, not a limitation. Every sensitive action goes through an Approved/ folder before execution. It's slow, but it means nothing happens that I didn't sign off on.

Separation of concerns matters more with agents. Perception, reasoning, and action should be separate processes. When your scraper crashes, your reasoner shouldn't go down with it.

The architecture looks like this:
Watcher → Needs_Action/ → Claude reasons → Pending_Approval/ → Human approves → Orchestrator executes → Done/

It's boring. It's file-based. It scales to my laptop.

And it works reliably in a way that monolithic "autonomous agent" scripts never did for me.

What design principles have you found most important when building production agent systems?
