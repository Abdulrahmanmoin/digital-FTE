---
type: linkedin_post_action
source_task: "scheduled_post"
status: pending_approval
---

# Proposed LinkedIn Post

Most developers think building an AI agent is about picking the right model.

It's not. It's about designing the right *loop*.

After spending weeks building autonomous AI agents that run as long-lived processes, here's what actually matters:

**State management beats prompt engineering.**
Your agent needs to know what it did yesterday. File-based state, persisted IDs, daily quotas — these boring engineering decisions are what separate a demo from a system that runs reliably at 3am.

**Separation of concerns is non-negotiable.**
Sense → Reason → Act. Keep these phases distinct. The watcher that reads your Gmail should never be the same process that sends a reply. Human approval gates belong in between.

**Files are underrated infrastructure.**
A folder called `Needs_Action/` that a human can open in Explorer is more auditable than any database dashboard. The simpler the interface, the more trust you build with the people using the system.

**Failure modes define your architecture.**
Browser crashes, session expiry, rate limits, network blips — design for all of them before you design for the happy path. Back-off logic and crash counters aren't optional extras.

I've been building a "Digital Employee" system using Playwright, Python, and Claude — and the hardest parts had nothing to do with AI.

They were classic software engineering problems wearing new clothes.

What's the most underestimated challenge you've faced building production AI systems?
